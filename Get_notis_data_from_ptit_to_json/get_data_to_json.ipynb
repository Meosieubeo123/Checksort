{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (5.2.1)\n",
      "Requirement already satisfied: bs4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml bs4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lấy thông tin từ portal.ptit.edu.vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global info_s_file\n",
    "global anal_info_s\n",
    "global contens_info_s\n",
    "\n",
    "info_s_file = 'request_notifications.json'\n",
    "anal_info_s = 'anal_notifications.json'\n",
    "contens_info_s = 'contens_notifications.json'\n",
    "\n",
    "max_page = 26 # Max is 116 with news ang 26 with notifications\n",
    "# base_url = 'https://portal.ptit.edu.vn/category/tin-tuc/'\n",
    "base_url = 'https://portal.ptit.edu.vn/category/thong-bao/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/bs4/__init__.py:228: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "all_news = []\n",
    "\n",
    "def get_info_s_from_portal(url):\n",
    "    global all_news\n",
    "    response = requests.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, from_encoding='utf-8', features='lxml')\n",
    "    divs = soup.find('div', class_='posts_group lm_wrapper classic col-3')\n",
    "    info_s = divs.find_all('div')\n",
    "    \n",
    "    for item in info_s:\n",
    "        info_item = {}\n",
    "        # Tìm tiêu đề từ class \"entry-title\"\n",
    "        title_tag = item.find('h2', class_='entry-title')\n",
    "        info_item['Title'] = title_tag.get_text().strip() if title_tag else ''\n",
    "        info_item['Link'] = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else ''\n",
    "\n",
    "        # Tìm nội dung từ class \"post-excerpt\"\n",
    "        excerpt_tag = item.find('div', class_='post-excerpt')\n",
    "        info_item['Content'] = excerpt_tag.get_text().strip() if excerpt_tag else ''\n",
    "\n",
    "        # Tìm ngày đăng từ class \"date_label\"\n",
    "        date_tag = item.find('div', class_='date_label')\n",
    "        info_item['Date'] = date_tag.get_text().strip() if date_tag else ''\n",
    "\n",
    "        all_news.append(info_item)\n",
    "\n",
    "def save_info_s_to_json():\n",
    "    with open(info_s_file, 'w', encoding='utf-8-sig') as file:\n",
    "        json.dump(all_news, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "for i in range(1, max_page + 1):\n",
    "    get_info_s_from_portal(base_url + 'page/' + str(i))\n",
    "\n",
    "save_info_s_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý dữ liệu JSON\n",
    "def non_empty_fields(item):\n",
    "    # Đếm số lượng trường không rỗng trong một item\n",
    "    non_empty_count = sum(1 for value in item.values() if value)\n",
    "    return non_empty_count, item\n",
    "\n",
    "def convert_date(date_str):\n",
    "    # Chuyển đổi ngày từ chuỗi sang đối tượng datetime, hoặc trả về None nếu không hợp lệ\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def process_json_data(data):\n",
    "    # Tạo list mới chỉ chứa items không hoàn toàn rỗng và thêm 'info_fullness' và 'Date' chuyển đổi\n",
    "    not_empty_data = [item for item in data if any(item.values())]\n",
    "    for item in not_empty_data:\n",
    "        item['info_fullness'] = non_empty_fields(item)[0]\n",
    "        item['Date'] = convert_date(item['Date']) if item['Date'] else None\n",
    "\n",
    "    # Sắp xếp list dựa trên 'info_fullness', sau đó theo 'Date' nếu có\n",
    "    not_empty_sorted = sorted(not_empty_data, key=lambda x: (-x['info_fullness'], x['Date'] if x['Date'] else datetime.min))\n",
    "\n",
    "    # Loại bỏ những items trùng lặp, giữ lại items với 'info_fullness' cao nhất\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    for item in not_empty_sorted:\n",
    "        # Tạo tuple của 'Title', 'Link' để kiểm tra trùng lặp\n",
    "        title_link = (item.get('Title'), item.get('Link'))\n",
    "        if title_link not in seen:\n",
    "            seen.add(title_link)\n",
    "            unique_data.append(item)\n",
    "\n",
    "    # Cuối cùng, sắp xếp kết quả dựa theo 'Date' giảm dần\n",
    "    final_data = sorted(unique_data, key=lambda x: x['Date'] if x['Date'] else datetime.min, reverse=True)\n",
    "    \n",
    "    # Loại bỏ field 'info_fullness' và item nào không có 'Date' hợp lệ\n",
    "    final_cleaned_data = [item for item in final_data if item['Date']]\n",
    "    # Chuyển đổi 'Date' thành chuỗi\n",
    "    for item in final_cleaned_data:\n",
    "        if item['Date']:\n",
    "            item['Date'] = item['Date'].strftime('%d/%m/%Y')\n",
    "\n",
    "    for item in final_cleaned_data:\n",
    "        del item['info_fullness']\n",
    "    \n",
    "    return final_cleaned_data\n",
    "\n",
    "# Chuẩn bị dữ liệu từ file JSON\n",
    "with open(info_s_file, 'r', encoding='utf-8-sig') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Xử lý dữ liệu\n",
    "cleaned_data = process_json_data(data)\n",
    "\n",
    "# Ghi dữ liệu đã được làm sạch vào file JSON mới\n",
    "with open(anal_info_s, 'w', encoding='utf-8-sig') as file:\n",
    "    json.dump(cleaned_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_from_page(url):\n",
    "    response = requests.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, from_encoding='utf-8', features='lxml')\n",
    "\n",
    "    div_contents = soup.find('div', class_='the_content_wrapper')\n",
    "    contents = div_contents.find_all('p')\n",
    "    \n",
    "    contens_ = ''\n",
    "    for content in contents:\n",
    "         contens_ = ' '.join([contens_, content.get_text().strip()])\n",
    "    return contens_\n",
    "\n",
    "# Đọc dữ liệu từ file JSON\n",
    "with open(anal_info_s, 'r', encoding='utf-8-sig') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Lấy nội dung từ mỗi liên kết và thêm vào mỗi item\n",
    "for item in data:\n",
    "    contens_info_s_link = item['Link']\n",
    "    new_content = get_contents_from_page(contens_info_s_link)\n",
    "    item['Content'] = new_content\n",
    "\n",
    "# Ghi dữ liệu làm sạch kèm nội dung vào file JSON mới\n",
    "with open(contens_info_s, 'w', encoding='utf-8-sig') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
